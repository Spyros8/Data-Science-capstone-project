{"cells": [{"metadata": {}, "cell_type": "code", "source": "\n!conda install -c conda-forge beautifulsoup4 --yes\n\n!conda install -c conda-forge geopy --yes\nimport folium\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nimport matplotlib.colors as colors\n!conda install -c conda-forge folium=0.5.0 --yes\nfrom sklearn.preprocessing import StandardScaler, normalize, scale\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom geopy.geocoders import Nominatim\nfrom bs4 import BeautifulSoup\nimport requests\nimport json\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Using Beautiful Soup to parse the website's html\ndata = requests.get('https://www.cityrealty.com/nyc/market-insight/features/get-to-know/average-nyc-condo-prices-neighborhood-june-2018/18804').text\nsoup = BeautifulSoup(data, 'html.parser')\n# Scrap the website tables for average prices\nareaList = []\nneighborhoodList = []\n\nfor area in soup.find_all(\"div\", class_=\"tile _quote _n1 _last\"):\n    areaText = area.find(\"a\").text\n    areaList.append(areaText)\n    \nfor index, table in enumerate(soup.find_all(\"table\", class_=\"table table-bordered table-hover table-condensed\")):\n    for row in table.find_all(\"tr\"):\n        cells = row.find_all(\"td\")\n        if len(cells) > 0:\n            neighborhoodName = cells[0].find(\"a\").text.strip()\n            avgPrice = cells[3].text.lstrip(\"$\").strip()\n            if \"K\" in avgPrice:\n                avgPrice = float(avgPrice.rstrip(\"K\")) * 1000\n            else: \n                if \"M\" in avgPrice:\n                    avgPrice = float(avgPrice.rstrip(\"M\")) * 1000000\n            \n            neighborhoodList.append((\n                areaList[index],\n                neighborhoodName,\n                avgPrice\n            ))\n            \n# Put the scrapped data into a dataframe\nnyc_neighborhoods_df = pd.DataFrame(neighborhoodList)\nnyc_neighborhoods_df.columns = ['Area', 'Neighborhood', 'AvgPrice']\nprint(nyc_neighborhoods_df.shape)\nnyc_neighborhoods_df.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "\n# Download the geodata\n!wget -q -O 'nyc_geo.json' https://ibm.box.com/shared/static/fbpwbovar7lf8p5sgddm06cgipa2rxpe.json\nprint('Data downloaded!')\nwith open('nyc_geo.json') as nyc_geo_json:\n    nyc_geo_data = json.load(nyc_geo_json)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Get the neighborhoods list\nnyc_geo_list = nyc_geo_data['features']\n\n# Sample neighborhood node\nnyc_geo_list[0]", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Parse the json data into neighborhoods list\nneighborhood_geo_list = []\nfor data in nyc_geo_list:\n    borough = neighborhood_name = data['properties']['borough'] \n    neighborhood_name = data['properties']['name']\n        \n    neighborhood_latlon = data['geometry']['coordinates']\n    neighborhood_lat = neighborhood_latlon[1]\n    neighborhood_lon = neighborhood_latlon[0]\n    \n    neighborhood_geo_list.append((\n        borough, neighborhood_name, neighborhood_lat, neighborhood_lon\n    ))\n    # Put into a dataframe\nneighborhood_geo_df = pd.DataFrame(neighborhood_geo_list)\nneighborhood_geo_df.columns = ['Borough', 'Neighborhood', 'Latitude', 'Longitude']\n\n# Avg price data is only available for Manhattan and Brooklyn\nneighborhood_geo_df = neighborhood_geo_df[(neighborhood_geo_df['Borough'] == 'Manhattan') | (neighborhood_geo_df['Borough'] == 'Brooklyn')]\n\nneighborhood_geo_df.reset_index(drop=True, inplace=True)\n\nprint(neighborhood_geo_df.shape)\nneighborhood_geo_df.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Bedford Stuyvesant missing a '-' in the middle\nneighborhood_geo_df.at[18, 'Neighborhood'] = 'Bedford-Stuyvesant'\n\n# Downtown is Downtown Brooklyn\nneighborhood_geo_df.at[41, 'Neighborhood'] = 'Downtown Brooklyn'\n\n# Dumbo should be DUMBO\nneighborhood_geo_df.at[104, 'Neighborhood'] = 'DUMBO'\n\n# Prospect Lefferts Gardens missing a '-' in the middle\nnyc_neighborhoods_df.at[15, 'Neighborhood'] = 'Prospect-Lefferts Gardens'\nneighborhood_geo_df.at[43, 'Neighborhood'] = 'Prospect-Lefferts Gardens'\n\n# South Slope - Greenwood Heights is just South Slope\nnyc_neighborhoods_df.at[17, 'Neighborhood'] = 'South Slope'\n# South Slope coordinates is missing\nneighborhood_geo_df = neighborhood_geo_df.append({'Borough': 'Brooklyn',\n                                                  'Neighborhood': 'South Slope',\n                                                  'Latitude': 40.662349, \n                                                  'Longitude': -73.990350}, ignore_index=True)\n\n# Park, Fifth Ave to 79th St is Upper East Side\nnyc_neighborhoods_df.at[24, 'Neighborhood'] = 'Upper East Side'\n# Flatiron/Union Square is just Flatiron\nnyc_neighborhoods_df.at[29, 'Neighborhood'] = 'Flatiron District'\nneighborhood_geo_df.at[99, 'Neighborhood'] = 'Flatiron District'\n\n# Gramercy Park is just Gramercy\nnyc_neighborhoods_df.at[30, 'Neighborhood'] = 'Gramercy'\n\n# NOHO should be just NoHo\nnyc_neighborhoods_df.at[33, 'Neighborhood'] = 'NoHo'\nneighborhood_geo_df.at[88, 'Neighborhood'] = 'NoHo'\n\n# NoLiTa/Little Italy is just NoLiTa\nnyc_neighborhoods_df.at[34, 'Neighborhood'] = 'NoLiTa'\nneighborhood_geo_df.at[76, 'Neighborhood'] = 'NoLiTa'\n\n# SOHO should be just SoHo\nnyc_neighborhoods_df.at[35, 'Neighborhood'] = 'SoHo'\nneighborhood_geo_df.at[77, 'Neighborhood'] = 'SoHo'\n\n# Stuyvesant Town / PCV is just Stuyvesant Town\nnyc_neighborhoods_df.at[36, 'Neighborhood'] = 'Stuyvesant Town'\n\n# Beekman/Sutton Place is just Sutton Place\nnyc_neighborhoods_df.at[39, 'Neighborhood'] = 'Sutton Place'\n# Midtown East and Midtown West will be combined into Midtown\nnyc_neighborhoods_df.at[40, 'Neighborhood'] = 'Midtown'\nmidtown_avg = (nyc_neighborhoods_df.at[40, 'AvgPrice'] + nyc_neighborhoods_df.at[41, 'AvgPrice']) / 2\nnyc_neighborhoods_df.at[40, 'AvgPrice'] = midtown_avg\nnyc_neighborhoods_df.at[41, 'AvgPrice'] = '-'\n\n# Turtle Bay/United Nations is just Turtle Bay\nnyc_neighborhoods_df.at[43, 'Neighborhood'] = 'Turtle Bay'\n\n# Central Harlem is Harlem\nneighborhood_geo_df.at[60, 'Neighborhood'] = 'Harlem'\n\n# Lincoln Center is Lincoln Square\nnyc_neighborhoods_df.at[51, 'Neighborhood'] = 'Lincoln Square'\n\n# Broadway Cooridor, Central Park West and Riverside Dr./West End Ave. will be combined to Upper West Side\nnyc_neighborhoods_df.at[49, 'Neighborhood'] = 'Upper West Side'\nmidtown_avg = (nyc_neighborhoods_df.at[49, 'AvgPrice'] + nyc_neighborhoods_df.at[50, 'AvgPrice'] + nyc_neighborhoods_df.at[53, 'AvgPrice']) / 3\nnyc_neighborhoods_df.at[49, 'AvgPrice'] = midtown_avg\nnyc_neighborhoods_df.at[50, 'AvgPrice'] = '-'\nnyc_neighborhoods_df.at[53, 'AvgPrice'] = '-'\n\n# Drop the Red Hook row\nnyc_neighborhoods_df.drop([16], inplace=True)\n# Inner join the two dataframes by Neighborhoods\nnyc_neighborhood_price_df = pd.concat([nyc_neighborhoods_df.set_index('Neighborhood'), neighborhood_geo_df.set_index('Neighborhood')], axis=1, join='inner')\nnyc_neighborhood_price_df.drop(columns=['Area', 'Borough'], inplace=True)\nnyc_neighborhood_price_df.reset_index(inplace=True)\n\n# The joined dataframe\nprint(nyc_neighborhood_price_df.shape)\nnyc_neighborhood_price_df.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# for choropleth map, we need another geo data which contain the Polygon type Coordinates\n!wget -q -O 'nyc_geo.geojson' http://data.beta.nyc//dataset/0ff93d2d-90ba-457c-9f7e-39e47bf2ac5f/resource/35dd04fb-81b3-479b-a074-a27a37888ce7/download/d085e2f8d0b54d4590b1e7d1f35594c1pediacitiesnycneighborhoods.geojson\nprint('Data downloaded!')\n\nnyc_polygon_geo_data = r'nyc_geo.geojson'\nlatitude = 40.8021285\nlongitude = -73.9777254\n# Map without markers\n\n# create a plain world map\nnyc_map = folium.Map(location=[latitude, longitude], zoom_start=11)\n\n# generate choropleth map\nnyc_map.choropleth(\n    geo_data=nyc_polygon_geo_data,\n    data=nyc_neighborhood_price_df,\n    columns=['Neighborhood', 'AvgPrice'],\n    key_on='feature.properties.neighborhood',\n    fill_color='YlOrRd', \n    fill_opacity=0.7, \n    line_opacity=0.2,\n    legend_name='Average 2 bedrooms condo price in New York city'\n)\n\n# display map\nnyc_map\n# Map with markers\n\n# create a plain world map\nnyc_map = folium.Map(location=[latitude, longitude], zoom_start=11)\n\n# generate choropleth map\nnyc_map.choropleth(\n    geo_data=nyc_polygon_geo_data,\n    data=nyc_neighborhood_price_df,\n    columns=['Neighborhood', 'AvgPrice'],\n    key_on='feature.properties.neighborhood',\n    fill_color='YlOrRd', \n    fill_opacity=0.7, \n    line_opacity=0.2,\n    legend_name='Average 2 bedrooms condo price in New York city'\n)\n# add markers to map\nfor lat, lng, neighborhood, price in zip(nyc_neighborhood_price_df['Latitude'], nyc_neighborhood_price_df['Longitude'], nyc_neighborhood_price_df['Neighborhood'], nyc_neighborhood_price_df['AvgPrice']):\n    label = '{}, ${:3.0f}'.format(neighborhood, price)\n    label = folium.Popup(label, parse_html=True)\n    folium.CircleMarker(\n        [lat, lng],\n        radius=5,\n        popup=label,\n        color='blue',\n        fill=True,\n        fill_color='#3186cc',\n        fill_opacity=0.7,\n        parse_html=False).add_to(nyc_map)\n\n# display map\nnyc_map", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "CLIENT_ID = 'LAD23Q55DYUE252MVVKRPWBH1S0CGSNXHZCUJHFFOWVCBEIH' \nCLIENT_SECRET = '0U2PWUZMMBK3JAMSK1PK5PWJN53OSFDABZUEPWUVBEIWXBQL' \nVERSION = '20180605' \nLIMIT = 30\n\nprint('Your credentails:')\nprint('CLIENT_ID: ' + CLIENT_ID)\nprint('CLIENT_SECRET:' + CLIENT_SECRET)\n# FourSquare parameters\nradius = 1000 # 1 km around the neighborhood center\nlimit = 200\n\nvenues = []\n\nfor lat, long, neighborhood in zip(nyc_neighborhood_price_df['Latitude'], nyc_neighborhood_price_df['Longitude'], nyc_neighborhood_price_df['Neighborhood']):\n    url = \"https://api.foursquare.com/v2/venues/explore?client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}\".format(\n        CLIENT_ID,\n        CLIENT_SECRET,\n        VERSION,\n        lat,\n        long,\n        radius, \n        limit)\n    \n    results = requests.get(url).json()[\"response\"]['groups'][0]['items']\n    \n    for venue in results:\n        venues.append((\n            neighborhood,\n            lat, \n            long, \n            venue['venue']['name'], \n            venue['venue']['location']['lat'], \n            venue['venue']['location']['lng'],  \n            venue['venue']['categories'][0]['name']))\n# put the venues into a dataframe\nvenues_df = pd.DataFrame(venues)\nvenues_df.columns = ['Neighborhood', 'Latitude', 'Longitude', 'VenueName', 'VenueLatitude', 'VenueLongitude', 'VenueType']\n\n# check the dataframe\nprint(venues_df.shape)\nprint('There are {} unique venue types.'.format(len(venues_df['VenueType'].unique())))\nvenues_df.head()\n# one hot encoding\nvenues_type_onehot = pd.get_dummies(venues_df[['VenueType']], prefix=\"\", prefix_sep=\"\")\n\n# add the neighborhood column\nvenues_type_onehot['Neighborhood'] = venues_df['Neighborhood']\nfix_columns = list(venues_type_onehot.columns[-1:]) + list(venues_type_onehot.columns[:-1])\nvenues_type_onehot = venues_type_onehot[fix_columns]\n\nprint(venues_type_onehot.shape)\nvenues_type_onehot.head()\n# get the occurrence of each venue type in each neighborhood\nvenue_count_df = venues_type_onehot.groupby(['Neighborhood']).sum().reset_index()\n\nprint(venue_count_df.shape)\nvenue_count_df.head()\n# get the standardized neighborhoods' average prices\nscaler = StandardScaler()\nstandardized_price = scaler.fit_transform(nyc_neighborhood_price_df[['AvgPrice']])\n\n# add the normalized price to the dataframe\nneighborhood_venues_withprice_df = pd.DataFrame(venue_count_df)\nneighborhood_venues_withprice_df['StandardizedAvgPrice'] = standardized_price\n\nprint(neighborhood_venues_withprice_df.shape)\nneighborhood_venues_withprice_df.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Using LinearRegression, we can get the list of coefficient correlations between each type of venue and the average price\nlreg = LinearRegression(normalize=True)\n\nX = neighborhood_venues_withprice_df.drop(columns=['Neighborhood', 'StandardizedAvgPrice'])\ny = neighborhood_venues_withprice_df['StandardizedAvgPrice']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\nmodel = lreg.fit(X_train, y_train)\n# let's see how well Linear Regression fit the problem\ny_pred = lreg.predict(X_test)\n\nprint('R2-score:', r2_score(y_test, y_pred)) # r2 score\nprint('Mean Squared Error:', mean_squared_error(y_test, y_pred)) # mse\n\nprint('Max positive coefs:', lreg.coef_[np.argsort(-lreg.coef_)[:10]])\nprint('Venue types with most postive effect:', X.columns[np.argsort(-lreg.coef_)[:10]].values)\nprint('Max negative coefs:', lreg.coef_[np.argsort(lreg.coef_)[:10]])\nprint('Venue types with most negative effect:', X.columns[np.argsort(lreg.coef_)[:10]].values)\ncoef_abs = abs(lreg.coef_)\nprint('Min coefs:', lreg.coef_[np.argsort(coef_abs)[:10]])\nprint('Venue types with least effect:', X.columns[np.argsort(coef_abs)[:10]].values)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "X = neighborhood_venues_withprice_df.drop(columns=['Neighborhood', 'StandardizedAvgPrice'])\ny = neighborhood_venues_withprice_df['StandardizedAvgPrice']\n\n# First, apply PCA\npca = PCA(svd_solver='auto', random_state=0)\nX_pca = pca.fit_transform(scale(X))\nn_component_list = range(1, 51)\nr2_list = []\nmse_list = []\n\n# Second, Linear Regression\nfor i in n_component_list:\n    lreg = LinearRegression()\n    X_train, X_test, y_train, y_test = train_test_split(X_pca[:,:i], y, test_size=0.2, random_state=0)\n    model = lreg.fit(X_train, y_train)\n    # check the result\n    y_pred = lreg.predict(X_test)\n    r2 = r2_score(y_test, y_pred) # r2 score\n    mse = mean_squared_error(y_test, y_pred) # mse\n    r2_list.append(r2)\n    mse_list.append(mse)\n    \nscores_df = pd.DataFrame.from_dict(dict([('NComponents', n_component_list),\n                                        ('R2', r2_list),\n                                        ('MSE', mse_list)]))\nscores_df.set_index('NComponents', inplace=True)\n# plot the scores to see the best n_components\nplt.subplot(1, 3, 1)\nscores_df['R2'].plot(kind='line')\nplt.title('R2 score / n components')\nplt.ylabel('R2 score')\nplt.xlabel('n components')\n\nplt.subplot(1, 3, 3)\nscores_df['MSE'].plot(kind='line')\nplt.title('MSE score / n components')\nplt.ylabel('MSE score')\nplt.xlabel('n components')\n\nplt.show()\n\nr2_max = scores_df['R2'].idxmax()\nprint(\"Best n:\", r2_max, \"R2 score:\", scores_df['R2'][r2_max])\n\nmse_min = scores_df['MSE'].idxmin()\nprint(\"Best n:\", mse_min, \"MSE:\", scores_df['MSE'][mse_min])\n# Use the best n_components parameter\nlreg = LinearRegression()\nX_train, X_test, y_train, y_test = train_test_split(X_pca[:,:r2_max], y, test_size=0.2, random_state=0)\nmodel = lreg.fit(X_train, y_train)\n\n# check the result\ny_pred = lreg.predict(X_test)\nr2 = r2_score(y_test, y_pred) # r2 score\nmse = mean_squared_error(y_test, y_pred) # mse\nprint(\"R2 score:\", r2)\nprint(\"MSE:\", mse)\n# Let's try to project the coefs back to the original number of features\neigenvectors = pca.components_\npcr_coefs = eigenvectors[:r2_max, :].T @ lreg.coef_\n\npcr_coefs.shape\n# Let's check which venue types effect the most and least\nprint('Max positive coefs:', pcr_coefs[np.argsort(-pcr_coefs)[:10]])\nprint('Venue types with most positive effect:', X.columns[np.argsort(-pcr_coefs)[:10]].values)\nprint('Max negative coefs:', pcr_coefs[np.argsort(pcr_coefs)[:10]])\nprint('Venue types with most negative effect:', X.columns[np.argsort(pcr_coefs)[:10]].values)\ncoef_abs = abs(pcr_coefs)\nprint('Min coefs:', pcr_coefs[np.argsort(coef_abs)[:10]])\nprint('Venue types with least effect:', X.columns[np.argsort(coef_abs)[:10]].values)", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.6", "language": "python"}, "language_info": {"name": "python", "version": "3.6.9", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 1}